# Daily check-in â€” 2025-01-30 (Friday)ğŸ’ªğŸ¼ğŸ’ªğŸ¼

## What I did today
- **Main goal:** Populate the database with millions of records for performance and scale testing.
- Analyzed different models and their relationships (e.g. one-to-many, foreign keys), then designed and wrote a data-population script that respects those relationships and constraints.
- Wrote a script that generates 50M+ records for load/scale testing.

## What I learned
- There are multiple ways to populate data (API, direct SQL, bulk insert scripts, etc.); the choice affects both speed and complexity.
- Populating via API is much slower than using direct database SQL or a SQL scriptâ€”for large volumes, going straight to the DB (or a script that runs SQL) is the better option.

## Constraints / additional information
- **Realistic relationship variety:** Data must reflect varied cardinality across related tables. Example: two tables with a 1:N relationship (e.g. Cars and Engines)â€”we need the dataset to include different ratios, not a fixed â€œ1 car : 1 engineâ€ everywhere.
  - Row 1: 1 car associated with 3 engines  
  - Row 2: 1 car associated with 2 engines  
  - Row 3: 1 car associated with 5 engines  
  So the script should produce this kind of variation throughout the data.
- **Priority:** Time is not the main constraint right now; the main task is getting the data in correctly, then running tests against that data.

## Work / projects
- **Performance testing:** Built a data-population script to generate 50M+ records for load/scale testing so the system can be validated before real data volume grows and to avoid performance issues at scale.
- Using AI (e.g. Copilot) to help write and refine the population logic and scripts.

## Blockers / notes
- **Open question:** Which method to use for populating (API vs. direct SQL vs. script) and how to ensure relationships are maintained correctly, including foreign key constraints and referential integrity, while still achieving the desired volume and variation.

