# Daily check-in — 2026-02-02 (Monday)

## What I did today
- Continued working on the data-population script for performance testing.
- First tried to write a single common script to handle data entry for multiple tables, starting from independent tables and then inserting into dependent tables that use foreign keys.
- While executing the script, hit several errors where the script was not correctly indexing or referencing parts of the codebase (e.g. models/tables/columns).
- After fixing those issues, ran into another set of errors due to mismatches between the application models and the database migration files.
- Switched to a different approach: instead of bulk inserts, focused on inserting a very small number of records (2–3 per table) to validate the logic and sequence before scaling up.

## What I learned
- A single \"one-size-fits-all\" script for many related tables is fragile—table order, relationships, and foreign keys make the logic more complex than it looks at first.
- For large datasets, it is safer to first get the script working on a small sample of records, confirm that the schema, models, and migrations are aligned, and only then increase the volume.
- Differences between models and migrations can easily break data scripts; keeping them in sync (and checking for missing or renamed columns) is critical before attempting bulk data loads.

## Work / projects
- Data population for performance testing: iterated on the script design and execution, moving from an ambitious all-in-one bulk approach to a smaller, safer, step-by-step approach to make sure relationships and constraints are correct.

## Blockers / notes
- New issue: noticing several missing or mismatched columns between the models, migrations, and the script. Need to reconcile these differences so the script can insert all required fields correctly before focusing on scaling up the data volume.

## Tomorrow
- Plan to fix the missing/mismatched columns, verify that all relationships and foreign key constraints are respected, and then start increasing the data volume once the small test inserts are fully stable.
